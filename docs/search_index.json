[["index.html", "Stocastic Chapter 1 Premilaries 1.1 Brownian Motion 1.2 Definition of Brownian Motion 1.3 Simple Properties of Brownian Motion 1.4 Wiener Integral", " Stocastic Ashan Jayamal 2024-08-18 Chapter 1 Premilaries Definition 1.1 Consider a set \\(X\\). An \\(\\sigma\\)-algebra \\(\\mathcal{F}\\) of subsets of \\(X\\) is a collection \\(\\mathcal{F}\\) of subsets of \\(X\\) satisfying the following conditions: \\(\\emptyset \\in \\mathcal{F}\\) If \\(B \\in \\mathcal{F}\\), then its complement \\(B^c\\) is also in \\(F\\) If \\(B_1, B_2, \\ldots\\) is a countable collection of sets in \\(\\mathcal{F}\\), then their union \\(\\bigcup_{n=1}^\\infty B_n\\) is also in \\(\\mathcal{F}\\). 1.1 Brownian Motion Let \\((\\Omega, F, P)\\) be a probability space. A stochastic process is a measurable function \\(X(t, \\omega)\\) defined on the product space \\([0,\\infty) \\times \\Omega\\). In particular: For each \\(t\\), \\(X(t, \\cdot)\\) is a random variable. For each \\(\\omega\\), \\(X(\\cdot, \\omega)\\) is a measurable function (called a sample path). For convenience, the random variable \\(X(t, \\cdot)\\) will be written as \\(X(t)\\) or \\(X_t\\). Thus, a stochastic process \\(X(t, \\omega)\\) can also be expressed as \\(X(t)(\\omega)\\) or simply as \\(X(t)\\) or \\(X_t\\). 1.2 Definition of Brownian Motion (#def:Brownian_Motaion) A stochastic process \\(B(t, \\omega)\\) is called a Brownian motion if it satisfies the following conditions: \\(P(\\{\\omega : B(0, \\omega) = 0\\}) = 1\\). For any \\(0 \\leq s &lt; t\\), the random variable \\(B(t) - B(s)\\) is normally distributed with mean 0 and variance \\(t - s\\), i.e., for any \\(a &lt; b\\), \\[ P(a \\leq B(t) - B(s) \\leq b) = \\frac{1}{\\sqrt{2\\pi(t - s)}} \\int_a^b e^{-x^2/2(t - s)} \\, dx. \\] \\(B(t, \\omega)\\) has independent increments, i.e., for any \\(0 \\leq t_1 &lt; t_2 &lt; \\ldots &lt; t_n\\), the random variables \\[B(t_1), B(t_2) - B(t_1), \\ldots, B(t_n) - B(t_{n-1})\\] are independent. Almost all sample paths of \\(B(t, \\omega)\\) are continuous functions, i.e., \\[P(\\{\\omega : B(\\cdot, \\omega) \\text{ is continuous}\\}) = 1\\]. 1.3 Simple Properties of Brownian Motion Let\\(B(t)\\) be a fixed Brownian motion. We give below some simple properties that follow directly from the definition of Brownian motion. Proposition 1.1 For any \\(t &gt; 0\\), \\(B(t)\\) is normally distributed with mean 0 and variance \\(t\\). For any \\(s, t \\geq 0\\), we have \\(\\mathbb{E}[B(s)B(t)] = \\min\\{s, t\\}\\). Remark. Regarding Definition @ref(def:Brownian_Motaion), it can be proved that condition (2) and E[B(s)B(t)] = min{s, t} imply condition (3). Proof. By condition (1), we have \\(B(t) = B(t)−B(0)\\) and so the first assertion follows from condition (2). With out loss of generlity, assume that \\(s&lt;t\\). \\[\\mathbb{E}[B(s)B(t)] = \\mathbb{E}[B(s)(B(t) - B(s)) + B(s)^2]= 0 + s = s\\] which is equal to \\(\\min\\{s, t\\}\\). Proposition 1.2 (Translation Invariance) For a fixed \\(t_0 \\geq 0\\), the stochastic process \\(B(t) = B(t + t_0) - B(t_0)\\) is also a Brownian motion. Proposition 1.3 (Scaling invariance) For any real number \\(\\lambda &gt; 0\\), the stochastic process \\(B(t) = \\frac{B(\\lambda t)}{\\sqrt{\\lambda}}\\) is also a Brownian motion. 1.4 Wiener Integral "],["introduction.html", "Chapter 2 Introduction 2.1 Events and Probability 2.2 Random Variables 2.3 Conditional Probability and Independence", " Chapter 2 Introduction 2.1 Events and Probability Definition 2.1 Let \\(\\Omega\\) be a non-empty set. A \\(\\sigma\\)-field \\(\\mathcal{F}\\) on \\(\\Omega\\) is a family of subsets of \\(\\Omega\\) such that: The empty set \\(\\emptyset\\) belongs to \\(\\mathcal{F}\\); If \\(A\\) belongs to \\(\\mathcal{F}\\), then so does the complement \\(\\Omega \\setminus A\\); If \\(A_1, A_2, \\ldots\\) is a sequence of sets in \\(\\mathcal{F}\\), then their union \\(A_1 \\cup A_2 \\cup \\cdots\\) also belongs to \\(\\mathcal{F}\\). Example 2.1 The family of Borel sets \\(\\mathcal{F}= B(\\mathbb{R})\\) s a \\(\\sigma\\)-field on IR. We recall that \\(B(\\mathbb{R})\\) is the smallest \\(\\sigma\\)-field containing all intervals in \\(\\mathbb{R}\\). Definition 2.2 Let \\(\\mathcal{F}\\) be a \\(\\sigma\\)-field on \\(\\Omega\\). A probability measure \\(P\\) is a function \\(P : \\mathcal{F} \\to [0, 1]\\) such that \\(P(\\Omega) = 1\\); if \\(A_1, A_2, \\ldots\\) are pairwise disjoint sets (that is, \\(A_i \\cap A_j = \\emptyset\\) for \\(i \\neq j\\)) belonging to \\(\\mathcal{F}\\), then \\[ P\\left(\\bigcup_{i=1}^{\\infty} A_i\\right) = \\sum_{i=1}^{\\infty} P(A_i); \\] The triple \\((\\Omega, \\mathcal{F}, P)\\) is called a probability space. \\ The sets belonging to \\(\\mathcal{F}\\) are called events. \\ An event \\(A\\) is said to occur almost surely (a.s.) whenever \\(P(A) = 1\\). Example 2.2 Let consider, \\(\\Omega=[0, 1]\\) with the \\(\\sigma\\)-field =\\(\\mathcal{F} = \\mathcal{B}([0, 1])\\) of Borel sets \\(B \\subseteq [0, 1]\\), and Lebesgue measure \\(P = \\text{Leb}\\) on \\([0, 1]\\). Then \\((\\Omega, \\mathcal{F}, P)\\) is a probability space. Recall that \\(\\text{Leb}\\) is the unique measure defined on Borel sets such that \\[\\text{Leb}[a, b] = b - a\\] for any interval \\([a, b]\\). (In fact, \\(\\text{Leb}\\) can be extended to a larger \\(\\sigma\\)-field, but we shall need Borel sets only.) Exercise 2.1 Show that if \\(A_1, A_2, \\ldots\\) is an expanding sequence of events, that is \\[ A_1 \\subseteq A_2 \\subseteq A_3 \\subseteq \\cdots \\] then \\[ P\\left(\\bigcup_{n=1}^{\\infty} A_n\\right) = \\lim_{n \\to \\infty} P(A_n). \\] Similarly, if \\(A_1, A_2, \\ldots\\) is a contracting sequence of events, that is, \\[ A_1 \\supseteq A_2 \\supseteq A_3 \\supseteq \\cdots \\] then \\[ P\\left(\\bigcap_{n=1}^{\\infty} A_n\\right) = \\lim_{n \\to \\infty} P(A_n). \\] Hint: Write \\(A_1 \\cup A_2 \\cup \\cdots\\) as the union of a sequence of disjoint events: start with \\(A_1\\), then add a disjoint set to obtain \\(A_1 \\cup A_2\\), then add a disjoint set again to obtain \\(A_1 \\cup A_2 \\cup A_3\\), and so on. Now that you have a sequence of disjoint sets, you can use the definition of a probability measure. To deal with the product \\(A_1 \\cap A_2 \\cap \\cdots\\), write it as a union of some events with the aid of De Morgan’s law. Lemma 2.1 (Borei-Cantelli) Let \\(A_1, A_2, \\ldots\\) be a sequence of events such that \\(P(A_1) + P(A_2) + \\cdots &lt; \\infty\\) and let \\(B_n = A_n \\cup A_{n+1} \\cup \\cdots\\). Then \\(P(B_1 \\cap B_2 \\cap \\cdots) = 0\\). Exercise 2.2 Prove the Borel-Cantelli lemma above. Hint: \\(B_1, B_2, \\ldots\\) is a contracting sequence of events. 2.2 Random Variables Definition 2.3 If \\(\\mathcal{F}\\) is a \\(\\sigma\\)-field on \\(\\Omega\\), then a function \\(X : \\Omega \\to \\mathbb{R}\\) is said to be \\(\\mathcal{F}\\)-measurable if \\[\\{\\omega \\in \\Omega : X(\\omega) \\in B\\}=X^{-1}(\\omega)\\] for every Borel set \\(B \\in \\mathcal{B}(\\mathbb{R})\\). If \\((\\Omega, \\mathcal{F}, P)\\) is a probability space, then such a function \\(X\\) is called a random variable. Definition 2.4 The \\(\\sigma\\)-field \\(\\sigma(X)\\) generated by a random variable \\(X : \\Omega \\to \\mathbb{R}\\) consists of all sets of the form \\(\\{\\omega \\in \\Omega : X(\\Omega)\\in B\\}\\), where \\(B\\) is a Borel set in \\(\\mathbb{R}\\). Definition 2.5 The \\(\\sigma\\)-field \\(\\sigma(\\{X_i : i \\in I\\})\\) generated by a family \\(\\{X_i : i \\in I\\}\\) of random variables is defined to be the smallest \\(\\sigma\\)-field containing all events of the form \\(\\{X_i \\in B\\}\\), where \\(B\\) is a Borel set in \\(\\mathbb{R}\\) and \\(i \\in I\\). Exercise 2.3 We call \\(f : \\mathbb{R} \\to \\mathbb{R}\\) a Borel function if the inverse image \\(f^{-1}(B)\\) of any Borel set \\(B\\) in \\(\\mathbb{R}\\) is a Borel set. Show that if \\(f\\) is a Borel function and \\(X\\) is a random variable, then the composition \\(f(X)\\) is \\(\\sigma(X)\\)-measurable. Hint: Consider the event \\(\\{f(X) \\in B\\}\\), where \\(B\\) is an arbitrary Borel set. Can this event be written as \\(\\{X \\in A\\}\\) for some Borel set \\(A\\)? Lemma 2.2 (Doob-Dynkin) Let \\(X\\) be a random variable. Then each \\(\\sigma(X)\\)-measurable random variable \\(\\eta\\) can be written as \\[ \\eta = f(X) \\] for some Borel function \\(f : \\mathbb{R} \\to \\mathbb{R}\\). Proof. Omiited Definition 2.6 Every random variable \\(X : \\Omega \\to \\mathbb{R}\\) gives rise to a probability measure \\[ P_X(B) = P\\{X \\in B\\} \\] on \\(\\mathbb{R}\\) defined on the \\(\\sigma\\)-field of Borel sets \\(B \\in \\mathcal{B}(\\mathbb{R})\\). We call \\(P_X\\) the distribution of \\(X\\). The function \\(F_X : \\mathbb{R} \\to [0, 1]\\) defined by \\[ F_X(x) = P\\{X \\leq x\\} \\] is called the cumulative distribution function (CDF) of \\(X\\). Exercise 2.4 Show that the distribution function \\(F\\) is non-decreasing, right-continuous, and \\[ \\lim_{x \\to -\\infty} F_{\\xi}(x) = 0, \\quad \\lim_{x \\to +\\infty} F_{\\Xi}(x) = 1. \\] For example, to verify right-continuity show that \\(F_{\\xi}(x_n) \\to F_{\\xi}(x)\\) for any decreasing sequence \\(x_n\\) such that \\(x_n \\to x\\). You may find the results of Exercises useful. Definition 2.7 If there is a Borel function \\(f: \\mathbb{R} \\to \\mathbb{R}\\) such that for any Borel set \\(B \\subset \\mathbb{R}\\) \\[ P\\{\\xi \\in B\\} = \\int_B f_\\xi(x) \\, dx, \\] then \\(\\xi\\) is said to be a random variable with absolutely continuous distribution and \\(f_\\xi\\) is called the density of \\(\\xi\\). If there is a (finite or infinite) sequence of pairwise distinct real numbers \\(x_1, x_2, \\ldots\\) such that for any Borel set \\(B \\subset \\mathbb{R}\\) \\[ P\\{\\xi \\in B\\} = \\sum_{x_i \\in B} P\\{\\xi = x_i\\}, \\] then \\(\\xi\\) is said to have a discrete distribution with values \\(x_1, x_2, \\ldots\\) and mass \\(P\\{\\xi = x_i\\}\\) at \\(x_i\\). Exercise 2.5 Suppose that \\(\\xi\\) has a continuous distribution with density \\(f\\). Show that \\(f\\) is continuous at \\(x\\). Express \\(F(x)\\) as an integral of \\(f\\). Show that if \\(\\xi\\) has discrete distribution with values \\(x_1, x_2, . . . ,\\) then \\(F_\\xi\\) is constant on each interval (s, t] not containing any of the x_i’s and has jumps of size P {= x_i} at each x_i· Hint The increment Fe ( t) - Fe ( s ) is equal to the total mass of the Xi’s that belong to the interval [s, t). Definition 2.8 The joint distribution of several random variables \\(\\xi_1, \\ldots, \\xi_n\\) is a probability measure \\(P_{\\xi_1, \\ldots, \\xi_n}\\) on \\(\\mathbb{R}^n\\) such that \\[P_{\\xi_1, \\ldots, \\xi_n}(B)=P\\left\\{\\xi_1, \\ldots, \\xi_n\\in B\\right\\}\\] for any Borel set \\(B\\) in \\(\\mathbb{R}^n\\). If there is a Borel function \\(f_{\\xi_1, \\ldots, \\xi_n} : \\mathbb{R}^n \\to \\mathbb{R}\\) such that \\[ P\\{(\\xi_1, \\ldots, \\xi_n) \\in B\\} = \\int_B f_{\\xi_1, \\ldots, \\xi_n}(x_1, \\ldots, x_n) \\, dx_1 \\cdots dx_n \\] for any Borel set \\(B\\) in \\(\\mathbb{R}^n\\), then \\(f_{\\xi_1, \\ldots, \\xi_n}\\) is called the joint density of \\(\\xi_1, \\ldots, \\xi_n\\). Definition 2.9 A random variable \\(\\xi : \\Omega \\to \\mathbb{R}\\) is said to be integrable if \\[ \\int_\\Omega |\\xi| \\, dP &lt; \\infty. \\] The integral \\[ \\mathbb{E}(\\xi) = \\int_\\Omega \\xi \\, dP \\] exists and is called the expectation of \\(\\xi\\). The family of integrable random variables \\(\\xi : \\Omega \\to \\mathbb{R}\\) will be denoted by \\(L^1\\) or, in case of possible ambiguity, by \\(L^1(\\Omega, \\mathcal{F}, P)\\). Example 2.3 The indicator function \\(\\mathbf{1}_A\\) of a set \\(A\\) is equal to 1 on \\(A\\) and 0 on the complement \\(\\Omega \\setminus A\\) of \\(A\\). i.e.: \\[1_A(a):=\\begin{cases} 1 &amp; \\text{ if } a \\in A \\\\0 &amp; \\text{ if } a \\not\\in A\\end{cases} \\] For any event \\(A\\), \\[\\mathbb{E}[1_A]=\\int_\\Omega 1_A dP=P(A)\\] we say that \\(\\eta : \\Omega \\to \\mathbb{R}\\) is a step function if \\[ \\eta = \\sum_{i=1}^n \\eta_i \\mathbf{1}_{A_i}, \\] where \\(\\eta_1, \\ldots, \\eta_n\\) are real numbers and \\(A_1, \\ldots, A_n\\) are pairwise disjoint events. Then, \\[\\mathbb{E}[\\eta]=\\int_\\Omega \\eta dP=\\sum_{i=1}^n\\eta_i \\int_\\Omega 1_{A_i} dP=\\sum_{i=1}^n \\eta_i P(A_i)\\] Exercise 2.6 Show that for any Borel function \\(h : \\mathbb{R} \\to \\mathbb{R}\\) such that \\(h(X)\\) is integrable, \\[ \\mathbb{E}(h(X)) = \\int h(x) \\, dP_X(x). \\] First verify the equality for step functions \\(h : \\mathbb{R} \\to \\mathbb{R}\\), then for non-negative ones by approximating them by step functions, and finally for arbitrary Borel functions by splitting them into positive and negative parts More to go … 2.3 Conditional Probability and Independence Definition 2.10 For any events \\(A, B \\in \\mathcal{F}\\) such that \\(P(B) \\neq 0\\), the conditional probability of \\(A\\) given \\(B\\) is defined by \\[P(A \\mid B) = \\frac{P(A \\cap B)}{P(B)}\\] Exercise 2.7 Prove the total probability formula for any event \\(A \\in \\mathcal{F}\\) and any sequence of pairwise disjoint events \\(B_1, B_2, \\ldots \\in \\mathcal{F}\\) such that \\(B_1 \\cup B_2 \\cup \\cdots = \\emptyset\\) and \\(P(B_n) \\neq 0\\) for any \\(n\\). Hint: \\(A = (A \\cap B_1) \\cup (A \\cap B_2) \\cup \\cdots\\) Definition 2.11 Two events \\(A, B \\in \\mathcal{F}\\) are called independent if \\[ P(A \\cap B) = P(A)P(B). \\] In general, we say that \\(n\\) events \\(A_1, \\ldots, A_n \\in \\mathcal{F}\\) are independent if for any indices \\(1 \\leq i_1 &lt; i_2 &lt; \\cdots &lt; i_k \\leq n\\), \\[ P(A_{i_1} \\cap A_{i_2} \\cap \\cdots \\cap A_{i_k}) = P(A_{i_1})P(A_{i_2}) \\cdots P(A_{i_k}). \\] Exercise 2.8 Let \\(P(B) \\neq 0\\). Show that \\(A\\) and \\(B\\) are independent events if and only if \\[ P(A \\mid B) = P(A). \\] If \\(P(B) \\neq 0\\), then you can divide by it. Definition 2.12 Two random variables \\(\\xi\\) and \\(\\eta\\) are called independent if for any Borel sets \\(A, B \\in \\mathcal{B}(\\mathbb{R})\\), the two events \\[ \\{ \\xi \\in A \\} \\text{ and } \\{ \\eta \\in B \\} \\] are independent. We say that \\(n\\) random variables \\(\\xi_1, \\ldots, \\xi_n\\) are independent if for any Borel sets \\(B_1, \\ldots, B_n \\in \\mathcal{B}(\\mathbb{R})\\), the events \\[ \\{ \\xi_1 \\in B_1 \\}, \\{ \\xi_2 \\in B_2 \\}, \\ldots, \\{ \\xi_n \\in B_n \\} \\] are independent. In general, a (finite or infinite) family of random variables is said to be independent if any finite number of random variables from this family are independent. Proposition 2.1 If two integrable random variables \\(\\xi, \\eta : \\Omega \\to \\mathbb{R}\\) are independent, then they are uncorrelated, i.e., \\[ E(\\xi \\eta) = E(\\xi) E(\\eta), \\] provided that the product \\(\\xi \\eta\\) is also integrable. If \\(\\xi_1, \\ldots, \\xi_n : \\Omega \\to \\mathbb{R}\\) are independent integrable random variables, then \\[ E(\\xi_1 \\xi_2 \\cdots \\xi_n) = E(\\xi_1) E(\\xi_2) \\cdots E(\\xi_n), \\] provided that the product \\(\\xi_1 \\xi_2 \\cdots \\xi_n\\) is also integrable. Definition 2.13 Two \\(\\sigma\\)-fields \\(\\mathcal{G}\\) and \\(\\mathcal{H}\\) contained in \\(\\mathcal{F}\\) are called independent if any two events \\[ A \\in \\mathcal{G} \\text{ and } B \\in \\mathcal{H} \\] are independent. Similarly, any finite number of \\(\\sigma\\)-fields \\(\\mathcal{G}_1, \\ldots, \\mathcal{G}_n\\) contained in \\(\\mathcal{F}\\) are independent if any \\(n\\) events \\[ A_1 \\in \\mathcal{G}_1, \\ldots, A_n \\in \\mathcal{G}_n \\] are independent. In general, a (finite or infinite) family of \\(\\sigma\\)-fields is said to be independent if any finite number of them are independent. Exercise 2.9 Show that two random variables \\(\\xi\\) and \\(\\eta\\) are independent if and only if the \\(\\sigma\\)-fields \\(\\sigma(\\xi)\\) and \\(\\sigma(\\eta)\\) generated by them are independent. The events in \\(\\sigma(\\xi)\\) and \\(\\sigma(\\eta)\\) are of the form \\(\\{\\xi \\in A\\}\\) and \\(\\{\\eta \\in B\\}\\), where \\(A\\) and \\(B\\) are Borel sets. Sometimes it is convenient to talk of independence for a combination of random variables and \\(\\sigma\\)-fields. Definition 2.14 We say that a random variable \\(\\xi\\) is independent of a \\(\\sigma\\)-field \\(\\mathcal{G}\\) if the \\(\\sigma\\)-fields \\[ \\sigma(\\xi) \\text{ and } \\mathcal{G} \\] are independent. This can be extended to any (finite or infinite) family consisting of random variables or \\(\\sigma\\)-fields or a combination of them both. Namely, such a family is called independent if for any finite number of random variables \\(\\xi_1, \\ldots, \\xi_m\\) and \\(\\sigma\\)-fields \\(\\mathcal{G}_1, \\ldots, \\mathcal{G}_n\\) from this family, the \\(\\sigma\\)-fields \\[\\sigma(\\xi_1),...,\\sigma(\\xi_m),\\mathcal{G}_1,..,\\mathcal{G}_n\\] are independent. "],["random-walk-to-brownier-motion..html", "Chapter 3 Random Walk to Brownier Motion.", " Chapter 3 Random Walk to Brownier Motion. Slandered approach model stochastic dynamic in discrete time. Let \\(\\eta_i\\) be an random variable on a conman probability space. We often \\(\\Omega,\\mathcal{F},P\\) assume that i.i.d. This case \\(\\eta _i\\) is called white noise, otherwise coloured noise. Now we have definite dynamics. It will be given as discreate time dynamical systems recursively by some non linear function. We define, \\[\\begin{equation} X_{n+1}=X_n+\\phi_{n+1}(X_n,\\eta_{n+1}) \\quad n=0,1,2,...\\tag{3.1} \\end{equation}\\] where \\(\\phi_n:\\mathbb{R}^d\\times \\mathbb{R}^d\\to \\mathbb{R}^d\\) are measurable. Further, if \\(X_0\\) and \\(\\eta_0\\) are all independent then \\(X_n\\) is called Markov Chain. Now let \\(\\eta_i\\) be i.i.d and defineda random walk \\[\\begin{align} S_n&amp;:=\\sum_{i=1}^n \\eta_i\\\\ S_{(n+1)}&amp;:=S_n+\\eta_{(n+1)} \\end{align}\\] We can rewrite (3.1) as \\[X_{n+1}-X_n=\\phi_{n+1}(X_n,S_{n+1}-S_n)\\quad n=0,1,...\\] This equation is called Stochastic difference equations. AIM: Develop a continuous time analogous. Question What to use an contionus time replacement of the random walks? Definition 3.1 Let \\(I\\) be index set \\((I=\\mathbb{N} \\text{ or } I=\\mathbb{R}^+)\\). A collection of random varibels \\((X_t)_{i\\in I}\\) on \\((\\Omega,\\mathcal{F},P)\\) is called staocastic process. We need \\(I\\) to be a just totally ordered set for convention of time. If it is not an totally ordered set it is not a stochastic process but a random field. Now we need a notation of a filtration. Definition 3.2 Le \\(\\mathcal{F}_t\\) be non-decreasing sequnce of sub sigma algbers of \\(\\mathcal{F}\\) (i.e. \\(\\mathcal{F}_s\\subseteq \\mathcal{F}_t\\) for all \\(s\\geq t, s,t\\in \\in I\\)), then \\((\\mathcal{F}_t)_{t\\in I}\\) is called a filtration. Last we need the notation adaptness. Definition 3.3 A stocticstic process \\(X_t\\) is called adapted to filteration \\((\\mathcal{F}_t)_t\\) if \\(X_t\\in \\mathcal{F}_t\\). i.e: \\(X_t\\) is measurable Theorem 3.1 (Central Limit Theorem) Let \\(Y_{n_i}:\\Omega \\to \\mathbb{R}^d\\) (be collection of random varibles), \\(1\\leq i \\leq n &lt; \\infty\\) be identical distributed and square intergable random varaible on \\((\\Omega,\\mathcal{F},P)\\) such that \\(Y_{n_1},Y_{n_2},...,Y_{n_n}\\) are independent for all \\(n\\in \\mathbb{N}\\). Then \\[\\left(\\frac{1}{\\sqrt{n}}\\sum_{i=1}^n Y_{n_i}-\\mathbb{E}[Y_i] \\right) \\xrightarrow{\\mathcal{D}} N(0,C) \\text{ as } n \\to \\infty\\], where\\(N(0,C)\\) is multivarible noremal distribution with covarice matrix \\[Y_{k,l}=Cov[Y^{(k)}_{n_i} -Y^{(l)}_{n_i}]\\] and\\(\\xrightarrow{\\mathcal{D}}\\) means “distribution is convergent” to ``` Proof. Omitted We consider the random walk \\[S_n=\\sum_{i=1}^n\\eta_i\\] with \\(\\eta_i\\in L^2(\\Omega,\\mathcal{F},P)\\) and normalized.(i.e. \\(\\mathbb{E}[\\eta_i]=0,Var[\\eta_i]=1\\)) Plotting (Linear Interpolation) This gives an idea about the existance of a scaling limit. Now a question might be rising. Question: What is right rescaling? That is we try to define a rescaled random walk \\(S^m_t\\)(Here superscipt \\(m\\) is for mesh size), \\((t=0,\\frac{1}{m},\\frac{2}{m},cdots)\\) with step-size \\(\\frac{1}{m}\\) \\[S_{\\frac{k}{m}}^{(m)}=c_mS_k\\] Here \\(here c_m\\) is rescaling constant. It is difficulit to correct \\(c_m\\), because unless it decay so fast at the end you convert to zero or blow up whole thing and goes to infinity. For \\(t=\\frac{k}{m}\\) we have \\[Var[S_t^{(m)}]=c^2_m\\] "],["conditional-expectation.html", "Chapter 4 Conditional Expectation 4.1 Conditioning on an Event 4.2 Conditioning on a Discrete Random Variable 4.3 Conditioning on an Arbitrary Random Variable", " Chapter 4 Conditional Expectation 4.1 Conditioning on an Event The first and simplest case to consider is that of the conditional expectation \\(\\mathbf{E} (\\xi|B)\\) of a random variable \\(\\xi\\) given an event \\(B\\). Definition 4.1 For any integrable random variable \\(\\xi\\) and any event \\(B \\in \\mathcal{F}\\) such that \\(P(B) \\neq 0\\), the conditional expectation of \\(\\xi\\) given \\(B\\) is defined by \\[ E(\\xi \\mid B) = \\frac{1}{P(B)} \\int_B \\xi \\, dP. \\] Example 4.1 Three coins, 10p, 20p, and 50p are tossed. The values of those coins that land heads up are added to work out the total amount \\(\\xi\\). What is the expected total amount \\(\\xi\\) given that two coins have landed heads up? Exercise 4.1 Show that \\(E(\\xi \\mid D) = E(\\xi)\\). Hint: The definition of \\(E(\\xi)\\) involves an integral and so does the definition of \\(E(\\xi \\mid D)\\). How are these integrals related? Exercise 4.2 Show that if \\[ \\mathbf{1}_A(\\omega) = \\begin{cases} 1 &amp; \\text{for } \\omega \\in A \\\\ 0 &amp; \\text{for } \\omega \\notin A \\end{cases} \\] (the indicator function of \\(A\\)), then \\[ E(\\mathbf{1}_A \\mid B) = P(A \\mid B), \\] where \\[ P(A \\mid B) = \\frac{P(A \\cap B)}{P(B)} \\] is the conditional probability of \\(A\\) given \\(B\\). Hint: Write \\(\\int_B \\mathbf{1}_A \\, dP\\) as \\(P(A \\cap B)\\). 4.2 Conditioning on a Discrete Random Variable The next step towards the general definition of conditional expectation involves conditioning by a discrete random variable \\(\\eta\\) with possible values \\(y_1, y_2, \\ldots\\) such that \\(P\\{\\eta = y_n\\} \\neq 0\\) for each \\(n\\). Finding out the value of \\(\\eta\\) amounts to finding out which of the events \\(\\{\\eta = y_n\\}\\) has occurred or not. Conditioning by \\(\\eta\\) should therefore be the same as conditioning by the events \\(\\{\\eta = y_n\\}\\). Because we do not know in advance which of these events will occur, we need to consider all possibilities, involving a sequence of conditional expectations \\[ E(\\xi \\mid \\{\\eta = y_1\\}), E(\\xi \\mid \\{\\eta = y_2\\}), \\ldots \\] A convenient way of doing this is to construct a new discrete random variable constant and equal to \\(E(\\xi \\mid \\{\\eta = y_n\\})\\) on each of the sets \\(\\{\\eta = y_n\\}\\). This leads us to the next definition. Definition 4.2 Let \\(\\xi\\) be an integrable random variable and let \\(\\eta\\) be a discrete random variable as above. Then the conditional expectation of \\(\\xi\\) given \\(\\eta\\) is defined to be a random variable \\(E(\\xi \\mid \\eta)\\) such that \\[ E(\\xi \\mid \\eta)(\\omega) = E(\\xi \\mid \\{\\eta = y_n\\}) \\text{ if } \\eta(\\omega) = y_n \\] for any \\(n = 1, 2, \\ldots\\). Example 4.2 Three coins, 10p, 20p, and 50p are tossed as in Example 4.1. What is the conditional expectation \\(E(\\xi \\mid \\eta)\\) of the total amount \\(\\xi\\) shown by the three coins given the total amount \\(\\eta\\) shown by the 10p and 20p coins only? Example 4.3 Take \\(\\Omega = [0, 1]\\) with the \\(\\sigma\\)-field of Borel sets and \\(P\\) the Lebesgue measure on \\((0, 1]\\). We shall find \\(E(\\xi \\mid \\eta)\\) for \\[ \\xi(x)=2x^2,~~~ \\eta(x) = \\begin{cases} 1 &amp; \\text{if } x \\in [0, \\frac{1}{3}], \\\\ 2 &amp; \\text{if } x \\in (\\frac{1}{3}, \\frac{2}{3}), \\\\ 0 &amp; \\text{if } x \\in (\\frac{2}{3}, 1]. \\end{cases} \\] The graph of \\(E(\\xi|\\eta)\\) is shown in following figure together with those of \\(\\xi\\) and \\(\\eta\\) Exercise 4.3 Show that if \\(\\eta\\) is a constant function, then \\(E(\\xi \\mid \\eta)\\) is constant and equal to \\(E(\\xi)\\). Hint: The event \\(\\{\\eta = c\\}\\) must be \\(\\emptyset\\) or \\(\\Omega\\) for any \\(c \\in \\mathbb{R}\\). Exercise 4.4 Show that \\[ E(\\mathbf{1}_A \\mid B)(\\omega) = \\begin{cases} P(A \\mid B) &amp; \\text{if } \\omega \\in B, \\\\ P(A \\mid \\Omega\\setminus B) &amp; \\text{if } \\omega \\notin B \\end{cases} \\] for any \\(B\\) such that \\(0 &lt; P(B) &lt; 1\\) Hint How many different values does \\(1_B\\) take? What are the sets on which these values are taken? Exercise 4.5 Assuming that \\(\\eta\\) is a discrete random variable, show that \\[ E(E(\\xi \\mid \\eta)) = E(\\xi). \\] Hint: Observe that \\[ \\int_B E(\\xi \\mid \\eta) \\, dP = \\int_B \\xi \\, dP \\] for any event \\(B\\) on which \\(\\eta\\) is constant. The desired equality can be obtained by covering \\(\\Omega\\) by countably many disjoint events of this kind. Proposition 4.1 If \\(\\xi\\) is an integrable random variable and \\(\\eta\\) is a discrete random variable, then: \\(E(\\xi \\mid \\eta)\\) is \\(\\sigma(\\eta)\\)-measurable; For any \\(A \\in \\sigma(\\eta)\\), \\[ \\int_A E(\\xi \\mid \\eta) \\, dP = \\int_A \\xi \\, dP. \\] Proof. Suppose that \\(\\eta\\) has pairwise distinct values \\(y_1, y_2, \\ldots\\). Then the events \\[ \\{\\eta = y_1\\}, \\{\\eta = y_2\\}, \\ldots \\] are pairwise disjoint and cover \\(\\Omega\\). The \\(\\sigma\\)-field \\(\\sigma(\\eta)\\) is generated by these events; in fact, every \\(A \\in \\sigma(\\eta)\\) is a countable union of sets of the form \\(\\{\\eta = y_n\\}\\). Because \\(E(\\xi \\mid \\eta)\\) is constant on each of these sets, it must be \\(\\sigma(\\eta)\\)-measurable. For each \\(n\\), we have \\[ \\int_{\\{\\eta = y_n\\}} E(\\xi \\mid \\eta) \\, dP = \\int_{\\{\\eta = y_n\\}} E(\\xi \\mid \\{\\eta = y_n\\}) \\, dP = \\int_{\\{\\eta = y_n\\}} \\xi \\, dP. \\] Since each \\(A \\in \\sigma(\\eta)\\) is a countable union of sets of the form \\(\\{\\eta = y_n\\}\\), which are pairwise disjoint, it follows that \\[ \\int_A E(\\xi \\mid \\eta) \\, dP = \\int_A \\xi \\, dP, \\] as required. 4.3 Conditioning on an Arbitrary Random Variable Definition 4.3 Let \\(\\xi\\) be an integrable random variable and let \\(\\eta\\) be an arbitrary random variable. Then the conditional expectation of \\(\\xi\\) given \\(\\eta\\) is defined to be a random variable \\(E(\\xi \\mid \\eta)\\) such that: \\(E(\\xi \\mid \\eta)\\) is \\(\\sigma(\\eta)\\)-measurable; For any \\(A \\in \\sigma(\\eta)\\), \\[ \\int_A E(\\xi \\mid \\eta) \\, dP = \\int_A \\xi \\, dP. \\] Remark. We can also define the conditional probability of an event \\(A \\in \\mathcal{F}\\) given () by \\[P(A|\\eta)=E(1_A|\\eta)\\] here \\(l_A\\) is the indicator function of A. Do the conditions of Definition 2.3 characterize \\(E (\\xi|\\eta)\\) uniquely? The lemma below implies that E \\(E (\\xi|\\eta)\\) is defined to within equality on a set of full measure. Namely, if \\[\\xi=\\xi&#39; \\text{ almost sure , then } E (\\xi|\\eta) = E (\\xi&#39;|\\eta) \\text{ almost sure}\\] An event \\(A\\) is said to occur almost surely (a.s.) whenever \\(P(A) = 1\\). The existence of \\(E (\\xi|\\eta)\\) will be discussed later in this chapter. Lemma 4.1 Let \\((\\Omega, \\mathcal{F}, P)\\) be a probability space and let \\(\\mathcal{G}\\) be a \\(\\sigma\\)-field contained in \\(\\mathcal{F}\\). If \\(\\xi\\) is a \\(\\mathcal{G}\\)-measurable random variable and for any \\(B \\in \\mathcal{G}\\), \\[ \\int_B \\xi \\, dP = 0, \\] then \\(\\xi = 0\\) a.s. "],["les.html", "Chapter 5 Les 5.1 Integrals 5.2 Random Walks", " Chapter 5 Les 5.1 Integrals First we review the definitions of the Riemann integral in calculus and the Riemann–Stieltjes integral in advanced calculus. 5.1.1 Riemann Integral Let \\(f\\) be an bounded function defined on a finite closed interval \\([a, b]\\). Then \\(f\\) is called Riemann integrable if the following limit exists. \\[\\begin{equation} x \\end{equation}\\] 5.2 Random Walks Consider a random walk starting at 0 with jumps \\(h\\) and \\(-h\\) equally likely at times \\(\\delta, 2\\delta,...\\), where \\(h,\\delta&gt;0\\) . More precisely, let \\(\\{X_n\\}_{n=1}^\\infty\\) be a sequence of independent and identically distributed random variables with \\[P(X_j = h) = P(X_j = −h) = \\frac{1}{2}\\] Let \\(Y_{\\delta,h}(0) =0\\) \\[Y_{\\delta,h}(n\\delta) = X_1 + X_2 + \\ldots + X_n\\] For \\(t &gt; 0\\) define \\(Y_{\\delta,h}(t)\\) by linearization: (i.e: For \\(n\\delta &lt; t &lt; (n + 1)\\delta\\), define \\[Y_{\\delta,h}(t) = \\frac{(n + 1)\\delta - t}{\\delta} Y_{\\delta,h}(n\\delta) + \\frac{t - n\\delta}{\\delta} Y_{\\delta,h}((n + 1)\\delta).\\] "],["book.html", "Chapter 6 Book", " Chapter 6 Book "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
